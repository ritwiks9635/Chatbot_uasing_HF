{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ritwiks9635/Chatbot_uasing_HF/blob/main/ChatBot_using_LSTM_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**ChatBot Model**\n",
        "\n",
        "[Dataset](https://www.kaggle.com/datasets/kausr25/chatterbotenglish)"
      ],
      "metadata": {
        "id": "AWZvwgGGO2qt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from zipfile import ZipFile\n",
        "data = \"/content/https:/www.kaggle.com/datasets/kausr25/chatterbotenglish/chatterbotenglish.zip\"\n",
        "with ZipFile(data,\"r\") as zip:\n",
        "  zip.extractall(\"Chatbot_data/data\")\n",
        "  print(\"the data has been extracted \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AOjMcpF3S16M",
        "outputId": "dc79ba22-5b5a-4b1d-8a4f-1843d2686d2f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the data has been extracted \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import yaml\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ],
      "metadata": {
        "id": "2rgqWaenPdjM"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dir_path = '/content/Chatbot_data/data'\n",
        "files_list = os.listdir(dir_path + os.sep)\n",
        "\n",
        "questions = []\n",
        "answers = []\n",
        "for filepath in files_list:\n",
        "    stream = open(dir_path + os.sep + filepath, \"rb\")\n",
        "    docs = yaml.safe_load(stream)\n",
        "    conversations = docs['conversations']\n",
        "    for con in conversations:\n",
        "        if len(con) > 2 :\n",
        "            questions.append(con[0])\n",
        "            replies = con[1 :]\n",
        "            ans = ''\n",
        "            for rep in replies:\n",
        "                ans += ' ' + rep\n",
        "            answers.append(ans)\n",
        "        elif len(con) > 1:\n",
        "            questions.append(con[0])\n",
        "            answers.append(con[1])\n",
        "\n",
        "answers_with_tags = []\n",
        "for i in range(len(answers)):\n",
        "    if type(answers[i]) == str:\n",
        "        answers_with_tags.append(answers[i])\n",
        "    else:\n",
        "        questions.pop(i)\n",
        "\n",
        "answers = []\n",
        "for i in range(len(answers_with_tags)) :\n",
        "    answers.append('<start> ' + answers_with_tags[i] + ' <end>')\n",
        "\n",
        "tokenizer = keras.preprocessing.text.Tokenizer()\n",
        "tokenizer.fit_on_texts(questions + answers)\n",
        "VOCAB_SIZE = len(tokenizer.word_index)+1\n",
        "print('VOCAB SIZE : {}'.format(VOCAB_SIZE))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ct9RZrrNT1_4",
        "outputId": "bec98d4f-e6af-401e-a1fa-b995a72d6c53"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VOCAB SIZE : 1894\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "import re"
      ],
      "metadata": {
        "id": "4iDr_aTWwuud"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = []\n",
        "for word in tokenizer.word_index:\n",
        "    vocab.append(word)\n",
        "\n",
        "\n",
        "q_tokenized = tokenizer.texts_to_sequences(questions)\n",
        "q_max_length = max([len(x) for x in q_tokenized])\n",
        "q_padded = keras.preprocessing.sequence.pad_sequences(q_tokenized, maxlen = q_max_length, padding  = \"post\")\n",
        "encoder_input_data = np.array(q_padded)\n",
        "print(encoder_input_data.shape, q_max_length)\n",
        "\n",
        "\n",
        "a_tokenized = tokenizer.texts_to_sequences(answers)\n",
        "a_max_length = max([len(x) for x in a_tokenized])\n",
        "a_padded = keras.preprocessing.sequence.pad_sequences(a_tokenized, maxlen = a_max_length, padding  = \"post\")\n",
        "decoder_input_data = np.array(a_padded)\n",
        "print(decoder_input_data.shape, a_max_length)\n",
        "\n",
        "\n",
        "a_tokenized = tokenizer.texts_to_sequences(answers)\n",
        "for i in range(len(a_tokenized)):\n",
        "    a_tokenized[i] = a_tokenized[i][1:]\n",
        "#a_max_length = max([len(x) for x in a_tokenized])\n",
        "a_padded = keras.preprocessing.sequence.pad_sequences(a_tokenized, maxlen = a_max_length, padding  = \"post\")\n",
        "one_hot_a = keras.utils.to_categorical(a_padded, VOCAB_SIZE)\n",
        "decoder_output_data = np.array(one_hot_a)\n",
        "print(decoder_output_data.shape)"
      ],
      "metadata": {
        "id": "FT_Cpmz3ylgc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "587bbd09-6fbf-4fc3-fe27-722743d6cd0e"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(564, 22) 22\n",
            "(564, 74) 74\n",
            "(564, 74, 1894)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Defining the Encoder-Decoder model**\n",
        "The model will have Embedding, LSTM and Dense layers. The basic configuration is as follows.\n",
        "- 2 Input Layers : One for encoder_input_data and another for decoder_input_data.\n",
        "- Embedding layer : For converting token vectors to fix sized dense vectors. ( Note : Don't forget the mask_zero=True argument here )\n",
        "- LSTM layer : Provide access to Long-Short Term cells.\n",
        "\n",
        "Working :\n",
        "\n",
        "1. The encoder_input_data comes in the Embedding layer ( encoder_embedding ).\n",
        "2. The output of the Embedding layer goes to the LSTM cell which produces 2 state vectors ( h and c which are encoder_states )\n",
        "3. These states are set in the LSTM cell of the decoder.\n",
        "4. The decoder_input_data comes in through the Embedding layer.\n",
        "5.The Embeddings goes in LSTM cell ( which had the states ) to produce seqeunces."
      ],
      "metadata": {
        "id": "Xm-bumnYbAPV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_inputs = layers.Input(shape=(q_max_length, ))\n",
        "encoder_embedding = layers.Embedding(VOCAB_SIZE, 200, mask_zero=True)(encoder_inputs)\n",
        "encoder_outputs ,state_h ,state_c = layers.LSTM(200 , return_state=True)(encoder_embedding)\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "decoder_inputs = layers.Input(shape=(a_max_length, ))\n",
        "decoder_embedding = layers.Embedding(VOCAB_SIZE, 200, mask_zero=True)(decoder_inputs)\n",
        "decoder_lstm = layers.LSTM(200 , return_state=True , return_sequences=True)\n",
        "decoder_outputs, _ , _ = decoder_lstm (decoder_embedding , initial_state=encoder_states)\n",
        "decoder_dense = layers.Dense(VOCAB_SIZE , activation= \"softmax\")\n",
        "output = decoder_dense(decoder_outputs)\n",
        "\n",
        "model = keras.Model([encoder_inputs, decoder_inputs], output)\n",
        "model.compile(optimizer=keras.optimizers.RMSprop(), loss='categorical_crossentropy')\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2awrJBqqIBtt",
        "outputId": "0fa6f7c2-4908-4acd-ffec-b628ac77fc10"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_13\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_15 (InputLayer)       [(None, 22)]                 0         []                            \n",
            "                                                                                                  \n",
            " input_16 (InputLayer)       [(None, 74)]                 0         []                            \n",
            "                                                                                                  \n",
            " embedding_2 (Embedding)     (None, 22, 200)              378800    ['input_15[0][0]']            \n",
            "                                                                                                  \n",
            " embedding_3 (Embedding)     (None, 74, 200)              378800    ['input_16[0][0]']            \n",
            "                                                                                                  \n",
            " lstm_2 (LSTM)               [(None, 200),                320800    ['embedding_2[0][0]']         \n",
            "                              (None, 200),                                                        \n",
            "                              (None, 200)]                                                        \n",
            "                                                                                                  \n",
            " lstm_3 (LSTM)               [(None, 74, 200),            320800    ['embedding_3[0][0]',         \n",
            "                              (None, 200),                           'lstm_2[0][1]',              \n",
            "                              (None, 200)]                           'lstm_2[0][2]']              \n",
            "                                                                                                  \n",
            " dense_1 (Dense)             (None, 74, 1894)             380694    ['lstm_3[0][0]']              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 1779894 (6.79 MB)\n",
            "Trainable params: 1779894 (6.79 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit([encoder_input_data , decoder_input_data], decoder_output_data, batch_size=50, epochs=150 )\n",
        "model.save( 'model.h5' )"
      ],
      "metadata": {
        "id": "FHD5ysHDNBFv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Defining inference models**\n",
        "We create inference models which help in predicting answers.\n",
        "\n",
        "**Encoder inference model** : Takes the question as input and outputs LSTM states ( h and c ).\n",
        "\n",
        "**Decoder inference model** : Takes in 2 inputs, one are the LSTM states ( Output of encoder model ), second are the answer input seqeunces ( ones not having the <start> tag ). It will output the answers for the question which we fed to the encoder model and its state values."
      ],
      "metadata": {
        "id": "_v_6rtODcNzk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_inference_models():\n",
        "\n",
        "    encoder_model = keras.Model(encoder_inputs, encoder_states)\n",
        "\n",
        "    decoder_state_input_h = layers.Input(shape=(200 ,))\n",
        "    decoder_state_input_c = layers.Input(shape=(200 ,))\n",
        "\n",
        "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "\n",
        "    decoder_outputs, state_h, state_c = decoder_lstm(decoder_embedding , initial_state=decoder_states_inputs)\n",
        "    decoder_states = [state_h, state_c]\n",
        "    decoder_outputs = decoder_dense(decoder_outputs)\n",
        "    decoder_model = keras.Model(\n",
        "        [decoder_inputs] + decoder_states_inputs,\n",
        "        [decoder_outputs] + decoder_states)\n",
        "\n",
        "    return encoder_model , decoder_model"
      ],
      "metadata": {
        "id": "ds4BhKbQOPVa"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def str_to_tokens(sentence : str):\n",
        "    words = sentence.lower().split()\n",
        "    tokens_list = list()\n",
        "    for word in words:\n",
        "        tokens_list.append(tokenizer.word_index[word])\n",
        "    return keras.preprocessing.sequence.pad_sequences([tokens_list], maxlen = q_max_length, padding='post')"
      ],
      "metadata": {
        "id": "tZRVaN7aO5-G"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. First, we take a question as input and predict the state values using enc_model.\n",
        "2. We set the state values in the decoder's LSTM.\n",
        "3.Then, we generate a sequence which contains the <start> element.\n",
        "4. We input this sequence in the dec_model.\n",
        "5. We replace the <start> element with the element which was predicted by the dec_model and update the state values.\n",
        "6. We carry out the above steps iteratively till we hit the <end> tag or the maximum answer length."
      ],
      "metadata": {
        "id": "tdP3a0T1ceoJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "enc_model , dec_model = make_inference_models()\n",
        "\n",
        "for _ in range(10):\n",
        "    states_values = enc_model.predict( str_to_tokens( input( 'Enter question : ' ) ) )\n",
        "    empty_target_seq = np.zeros( ( 1 , 1 ) )\n",
        "    empty_target_seq[0, 0] = tokenizer.word_index['start']\n",
        "    stop_condition = False\n",
        "    decoded_translation = ''\n",
        "    while not stop_condition :\n",
        "        dec_outputs , h , c = dec_model.predict([ empty_target_seq ] + states_values )\n",
        "        sampled_word_index = np.argmax( dec_outputs[0, -1, :] )\n",
        "        sampled_word = None\n",
        "        for word , index in tokenizer.word_index.items() :\n",
        "            if sampled_word_index == index :\n",
        "                decoded_translation += ' {}'.format( word )\n",
        "                sampled_word = word\n",
        "\n",
        "        if sampled_word == 'end' or len(decoded_translation.split()) > a_max_length:\n",
        "            stop_condition = True\n",
        "\n",
        "        empty_target_seq = np.zeros( ( 1 , 1 ) )\n",
        "        empty_target_seq[ 0 , 0 ] = sampled_word_index\n",
        "        states_values = [ h , c ]\n",
        "\n",
        "    print( decoded_translation )"
      ],
      "metadata": {
        "id": "RNJ-31FmaRjU"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}